{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date     SKU  Sales  Price  Inventory    Category\n",
      "1820 2023-12-28  SKU005   3255  15.96        274  Home Decor\n",
      "1821 2023-06-20  SKU002   6281   9.71        170       Books\n",
      "1822 2023-01-14  SKU003   2782   9.29        769  Home Decor\n",
      "1823 2023-12-12  SKU002   3399  28.44        759       Books\n",
      "1824 2023-12-21  SKU004   5385  34.95        782    Clothing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Create a list of unique SKUs\n",
    "sku_list = [\"SKU001\", \"SKU002\", \"SKU003\", \"SKU004\", \"SKU005\"]\n",
    "\n",
    "# Create a list of dates for the entire year\n",
    "dates = pd.date_range(start='2023-01-01', periods=365, freq='D')\n",
    "\n",
    "# Generate random sales data for each SKU\n",
    "data = {\n",
    "    'Date': [random.choice(dates) for _ in range(365 * len(sku_list))],\n",
    "    'SKU': [random.choice(sku_list) for _ in range(365 * len(sku_list))],\n",
    "    'Sales': [random.randint(1000, 10000) for _ in range(365 * len(sku_list))],\n",
    "    'Price': [round(random.uniform(5, 50), 2) for _ in range(365 * len(sku_list))],\n",
    "    'Inventory': [random.randint(100, 1000) for _ in range(365 * len(sku_list))],\n",
    "    'Category': [random.choice(['Electronics', 'Clothing', 'Home Decor', 'Toys', 'Books']) for _ in range(365 * len(sku_list))]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "sales_data = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the sales data with attributes\n",
    "print(sales_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Create a One-Class SVM model\n",
    "clf = OneClassSVM(nu=0.05, kernel=\"rbf\", gamma=0.1)\n",
    "\n",
    "# Fit the model to the sales data\n",
    "clf.fit(sales_data[['Sales']])\n",
    "\n",
    "# Predict anomalies\n",
    "anomalies = clf.predict(sales_data[['Sales']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a DBSCAN model\n",
    "dbscan = DBSCAN(eps=1000, min_samples=5)\n",
    "\n",
    "# Fit the model to the sales data\n",
    "labels = dbscan.fit_predict(sales_data[['Sales']])\n",
    "\n",
    "# Label -1 represents outliers/anomalies\n",
    "anomalies = labels == -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means model with a suitable number of clusters\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "# Fit the model to the sales data\n",
    "sales_data['Cluster'] = kmeans.fit_predict(sales_data[['Sales']])\n",
    "\n",
    "# Anomalies are data points that don't belong to any cluster (cluster -1)\n",
    "anomalies = sales_data['Cluster'] == -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCF\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create an Isolation Forest model\n",
    "clf_iforest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the sales data\n",
    "X = np.array(sales_data['Sales']).reshape(-1, 1)\n",
    "clf_iforest.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "outliers_iforest = clf_iforest.predict(X)\n",
    "\n",
    "# Convert the Isolation Forest output to Boolean values\n",
    "anomalies_iforest = outliers_iforest == -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_iforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies_iforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 865us/step\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "# For LSTM-based anomaly detection, you would typically need a time series dataset with sequences. \n",
    "# Here's a basic example of using LSTM for sequence-based anomaly detection:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Create sequences of sales data\n",
    "seq_length = 10\n",
    "sequences = [np.array(sales_data['Sales'][i:i+seq_length]) for i in range(len(sales_data) - seq_length + 1)]\n",
    "\n",
    "# Reshape data for LSTM\n",
    "X = np.array(sequences)\n",
    "\n",
    "# Create an LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(seq_length, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "predictions = model.predict(X)\n",
    "# Calculate reconstruction errors\n",
    "reconstruction_errors = np.mean(np.square(X - predictions), axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "threshold = np.percentile(reconstruction_errors, 95)\n",
    "anomalies_lstm = reconstruction_errors > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM-PCA:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(sales_data[['Sales']])\n",
    "\n",
    "# Create a Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
    "\n",
    "# Fit the model to the reduced data\n",
    "gmm.fit(X_pca)\n",
    "\n",
    "# Predict anomalies using Mahalanobis distance\n",
    "mahal = gmm.score_samples(X_pca)\n",
    "threshold = np.percentile(mahal, 5)\n",
    "anomalies_gmm_pca = mahal < threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_gmm_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies_gmm_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 376us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Create sequences of sales data\n",
    "seq_length = 10\n",
    "sequences = [np.array(sales_data['Sales'][i:i+seq_length]) for i in range(len(sales_data) - seq_length + 1)]\n",
    "\n",
    "# Reshape data for Autoencoder\n",
    "X = np.array(sequences)  # Shape should be (number of sequences, seq_length)\n",
    "\n",
    "# Create an Autoencoder model\n",
    "input_layer = Input(shape=(seq_length,))\n",
    "encoded = Dense(10, activation='relu')(input_layer)\n",
    "decoded = Dense(seq_length, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the model to the sales data\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "predictions = autoencoder.predict(X)\n",
    "reconstruction_errors_autoencoder = np.mean(np.square(X - predictions), axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "threshold = np.percentile(reconstruction_errors_autoencoder, 95)\n",
    "anomalies_autoencoder = reconstruction_errors_autoencoder > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False, False, False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
