- [Introduction](#introduction)
  - [Definition of Outliers / Anomalies](#definition-of-outliers--anomalies)
  - [Time Series Anomaly Detection](#time-series-anomaly-detection)
  - [Two Tracks: Supervised \& Unsupervised Learning](#two-tracks-supervised--unsupervised-learning)
- [Clutering](#clutering)
  - [K-Means](#k-means)
  - [FCM Clustering](#fcm-clustering)
  - [Major Disadvantages of Clustering Algorithms and Solutions](#major-disadvantages-of-clustering-algorithms-and-solutions)
- [EM / GMM](#em--gmm)
- [Auto-Encoder with Regressions](#auto-encoder-with-regressions)
  - [Prediction based vs.Reconstruction based](#prediction-based-vsreconstruction-based)
  - [Prediction-based approaches](#prediction-based-approaches)
    - [ARIMA](#arima)
    - [LSTM-DT](#lstm-dt)
  - [Reconstruction-based approaches](#reconstruction-based-approaches)
    - [PCA](#pca)
    - [LSTM-AE](#lstm-ae)
    - [LSTM-VAE](#lstm-vae)
    - [GAN](#gan)


## Introduction

Finding outliers, rare events from a collection of patterns, has become an emerging issue in the area of machine learning concerned with detecting for further investigation and eventually removing anomalous objects in data. A key challenge with outliers / anomalies detection is because they are not a well-formulated issue. (A carcity of labeled data anbd ambiguous defiition of anomalies. )

Outliers are defined as the extreme values that deviate from the overall patterns in data. Detecting outliers or anomalies can lead to discovery of hidden knowledge. (Abinakt detection, an important task within time series analysis, explicitly aims to identify unexpected events.)

There are many techniques for detecting outliers or anomalies, but no single technique is proven to be the standard universal choice. Depending on the nature, different implementations require the use of different outlier detection methods. 

### Definition of Outliers / Anomalies

We define outlier in the database as an observation that deviates significantly from the majority of patterns and stirs suspicion that a different mechanism generated the outlier. Generally, outliers/anomalies detection singles out those objects, which are significantly deviating from the rest of the dataset to the extent that they seem to be generated by another process. Outlier tends to bias statistical estimators, and they are the result of many artificial intelligence methods like the standard deviation, the mean value, or the position of the prototypes of k-means algorithms is affected. 

Note that statistical estimates like the standard deviation and arithmetic mean might be affected by the spread of the data-points that lie far from the middle of the data distribution. 

Understanding the outlier type will significantly affect the way to approach anomalies. A univariate outlier is a data point with an extreme value of the variable. Univariate outliers are often found when the distribution of values is viewed in a single space. A multivariate outlier, on the other hand, is a combination of unusual values on at least two independent or dependent variables found in n-dimensional spaces.

### Time Series Anomaly Detection

While the criteria differ across domains, anomalies in time
series typically exhibit one of three identifiable patterns: point,contextual, or collective. Point anomalies are singular data points that suddenly deviate from the normal range of the series. An example might be a sudden an unexpected spike in sales, potentially due to a unique promotion event. Collective anomalies are a series
of consecutive data points that are considered anomalous as a whole. Finally, contextual anomalies are groups of data points that fall within the seriesâ€™ normal range but do not follow expected temporal patterns, such as irregular purchasing patterns during certain promotions or marketing events.





### Two Tracks: Supervised & Unsupervised Learning

Supervised and unsupervised learning methods are the two distinct fundamental machine learning approaches to the problem of anomalies/outliers detection. The Supervised learning method builds a model on some norm, such as labeled outliers types, and detect deviations in observed (labeled) data from the normal model. The advantage of a supervised anomaly/outlier detection method is that it detects new types of activity as deviations from normal usage. In contrast, an unsupervised method learns hidden structures from unlabeled data, and identifies outliers/anomalies without the use of prior knowledge of the data and requires that the analyst or decision-maker has the capacity to interpret the obtained results correctly and use them to make the right decisions. Outlier detection methods are often used for finding sudden or unexpected changes in data in the early stages of the analysis process. Unsupervised learning, such as clustering algorithms, uses unlabeled input data and allows the algorithm to act on that information without guidance (training). In clustering, the task is to assign a set of data objects into groups (clusters) so that the data objects in the cluster are in some sense more like each other, and different from data objects in other groups (clusters).

Other regression based unsupervised learning methods have made remarkable progress in tackling the time series anomaly detection using either single-timestamp predictions or time series reconstructions. 

- AER: Auto-encoder with regression. A joint model that combines a vanilla auto-encoder and an LSTM regressor. 
- prediction based 1
- prediction based 2
- reconstruction based 1
- reconstruction based 2
- reconstruction based 3










## Clutering

The clustering method is a very powerful method in the field of machine learning and defines outliers in terms of their distance to the cluster centers. 

In this section, we mainly focus on the clustering-based approach to identify outliers or anomalies in the retail sales data. To select the best clustering algorithm for the purpose, two algorithms are applied, K-means for hard, crip clustering and Fuzzy C-means (FCM) for soft clustering. The results show that the K-means algorithm outperforms the FCM algorithm in terms of outlier detection efficiency, and it is an effecytive outlier detection solution. 

### K-Means 

KM is a non-hierarchical, partitional, distance-based clustering algorithm and is primarily suitable for large databases commonly used in the field of merchandizing. The algorithm has been extensively used in market / customer segmentation and sales patterns recognition because of simplicity in implementation, its stable performance, and fast execution. KM algorithm's main objective is to partition $n$ objects into $k$ clusters so that the inter-cluster similarity as the distance between observations is minimum and intra-cluster similarity is maximum. 

### FCM Clustering 

In the FCM algorithm one data point to belong to two or more groups (clusters) with a fuzzy membership, using concepts from the field of fuzzy set theory and fuzzy logic. Assigning each data point $i$ a membership to any of the $j = 1,..,k$ clusters with membership value $\mu_j = [0, 1]$ determines the degree of belonging to the cluster $j$.

Accordingly, memberships of the data point $i$ to $k$ clusters are shown with vector $u_{ij}$. Allowing for membership in several clusters serves our purpose of finding outliers, as data point i with several $\mu_{ij} > 0$ may indicate outliers in the spaces between cluster centers. This adds a powerful detection capability compared to traditional hard-threshold clustering, such as KM, where every point is assigned a crisp, exact label $l$ as the membership to the assigned cluster and 0 to all the rest of $k-1$ clusters. More generally, a data point close to the cluster center has a high-degree of non-exclusive membership in that cluster and generally has lower memberships in other clusters, while for data point farther away from the cluster center the non-exclusive membership in the cluster in question is lower but may well have memberships in other clusters as well. However, the sum of the membership values of a data point to all clusters must be 1.

### Major Disadvantages of Clustering Algorithms and Solutions

1. Sensitivity to Outliers:
   - Clustering algorithms can be sensitive to outliers, and anomalies might be wrongly assigned to a cluster or create their own cluster. Outliers can significantly impact the centroid and shape of clusters.
   - Solutions:
     - K-Means: identify outliers in the context of clusters and remove them during training time.
     - Use Robust Clustering Algorithms (DBSCAN) where outliers are considered as noise and not assigned to any cluster.
     - Use Dynamic or Online Clustering (MiniBatchKMeans)
2. Need for Predefined Number of Clusters
   - Many clustering algorithms require specifying the number of clusters beforehand (k in k-means). Determining the optimal number of clusters is often a challenging task and might vary for different datasets.
   - Solutions:
     - Elbow Method
     - Silhouette Score: Compute the silhouette score for different numbers of clusters and choose the number that maximizes the silhouette score.
3. Scalability Issues
   - Clustering algorithms may not scale well to large datasets, and the computational cost can become a limiting factor, especially for real-time or streaming data.
   - Solutions:
     - MiniBatchKMeans: As mentioned earlier, use MiniBatchKMeans for large datasets.
     - Parallelization: Some clustering algorithms, like DBSCAN, support parallelization. You can set the n_jobs parameter to use multiple CPU cores.
     - Sampling: For very large datasets, consider using a representative sample for clustering rather than the entire dataset.


## EM / GMM

## Auto-Encoder with Regressions

### Prediction based vs.Reconstruction based

Existing machine learning methods for anomaly detection
on time series can be either prediction-based or reconstruction based. Prediction-based methods train a model to learn
previous patterns in order to forecast future observations.
An observation is anomalous when the predicted value deviates
significantly from the actual value. Prediction-based methods
are good at revealing point anomalies but tend to produce
more *false detection*. On the other hand, reconstruction based
methods learn a latent low-dimensional representation
to reconstruct the original input. This method assumes
that anomalies are rare events that are lost in the mapping
to the latent space. Hence, regions that cannot be effectively
reconstructed are considered anomalous. In some experiments,
the researchers observed that reconstruction-based methods tend to be
more effective than prediction-based methods at identifying
*contextual and collective anomalies*.

### Prediction-based approaches

Prediction based approaches use the deviation between the predicted and actual values to identify anomalies. 

#### ARIMA

ARIMA uses lags and lagged forecast errors to predict future values. 


#### LSTM-DT

In the modeling stage, the method uses a separate
LSTM neural network to model each channel in order to
facilitate granular system control and mitigate errors from
high-dimensionality outputs. In the post-processing stage, the method combines an exponentially-weighted average function with a non-parametric dynamic thresholding technique to
detect anomalous intervals.

### Reconstruction-based approaches

Reconstruction-based approaches learn a latent lowdimensional
representation to reconstruct the original input.
These methods assume that the latent space prioritizes capturing
common patterns within the dataset. Rare events like
anomalies are not captured in the latent representation and
are less likely to be accurately reconstructed.

#### PCA

PCA is a dimensionality-reduction technique limited to linear reconstructions
and fails to leverage spatial-temporal correlation
in multivariate settings.

#### LSTM-AE

LSTM-AE is an auto-encoder built
from LSTM layers that learns a latent space representation for
the input. The size of the latent space needs to be calibrated
to capture generalizable patterns while avoiding noise and
anomalies.

#### LSTM-VAE

LSTM-VAE introduces regularization in the latent
space using a probabilistic encoder and decoder. However,
these methods tend to overfit to the training data, which results
in decreased performances.

#### GAN